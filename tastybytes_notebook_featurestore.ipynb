{
 "metadata": {
  "hex_info": {
   "author": "Michelle Li",
   "exported_date": "Sun Apr 28 2024 18:35:34 GMT+0000 (Coordinated Universal Time)",
   "project_id": "b215be44-db76-4c2d-a4cf-d34051f64ca7",
   "version": "draft"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "orig_nbformat": 4
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69049a47-a548-45c1-bebd-72e9247a3624",
   "metadata": {
    "codeCollapsed": true,
    "collapsed": false,
    "language": "python",
    "name": "banner",
    "resultHeight": 241
   },
   "outputs": [],
   "source": [
    "import streamlit as st;\n",
    "st.image('https://raw.githubusercontent.com/Snowflake-Labs/sfguide-tasty-bytes-snowpark-101-for-data-science/211360e75995b670b2b25d3204b59b42e48456cd//assets/Banner-3.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216726bf-c89b-4962-b45d-6c7752a1151e",
   "metadata": {
    "collapsed": false,
    "name": "HOL_Overview",
    "resultHeight": 649
   },
   "source": [
    "# Overview\n",
    "\n",
    "Tasty Bytes is one of the largest food truck networks in the world with localized menu options spread across 30 major cities in 15 countries.\n",
    "**Tasty Bytes is aiming to achieve 25% YoY sales growth over 5 years.**\n",
    "\n",
    "As Tasty Bytes Data Scientists, we have been asked to support this goal by helping our food truck drivers more intelligently pick where to park for shifts.\n",
    "**We want to direct our trucks to locations that are expected to have the highest sales on a given shift. This will maximize our daily revenue across our fleet of trucks.**\n",
    "\n",
    "To provide this insight, we will use historical shift sales at each location to build a model. This data has been made available to us in Snowflake.\n",
    "Our model will provide the predicted sales at each location for the upcoming shift.\n",
    "\n",
    "\n",
    "\n",
    "#### **This is an introduction to Snowpark for Snowflake. We will use Snowpark to:**\n",
    "\n",
    "- Explore the data\n",
    "- Perform feature engineering\n",
    "- Train a model\n",
    "- Deploy the model in Snowflake\n",
    "\n",
    "**Why Snowpark?**\n",
    "\n",
    "- No copies or movement of data\n",
    "- Maintain governance\n",
    "- Leverage Snowflake scalable compute\n",
    "- ...and more!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe21437-ef72-43ca-bfa5-a8a8c612cfe0",
   "metadata": {
    "codeCollapsed": true,
    "collapsed": false,
    "language": "python",
    "name": "Snowpark",
    "resultHeight": 0
   },
   "outputs": [],
   "source": [
    "import streamlit as st;\n",
    "st.image('https://raw.githubusercontent.com/Snowflake-Labs/sfguide-tasty-bytes-snowpark-101-for-data-science/211360e75995b670b2b25d3204b59b42e48456cd//assets/snowpark_101.png')\n",
    "st.markdown(f\"Let\\'s get to know Snowpark. We will see that Snowpark makes it easy for Python users to leverage the Snowflake platform. Bringing these users into the Snowflake platform will foster collaboration and streamline architecture across all users and teams.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846843ba-a6d1-47c6-8503-c0c0d7d3d707",
   "metadata": {
    "collapsed": false,
    "name": "Libraries",
    "resultHeight": 143
   },
   "source": [
    "## Import Packages\n",
    "\n",
    "Just like the Python packages we are importing, we will import the Snowpark modules that we need.\n",
    "\n",
    "**Value**: Snowflake modules provide efficient ways to work with data and functions in Snowflake.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08df2ef6-279b-492c-b787-9fb48e123197",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "Import_Packages",
    "resultHeight": 38
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Imports complete.\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "# Import Python packages\nimport pandas as pd\nimport json\nimport sys\nimport cachetools\nimport modin.pandas as pd\nimport snowflake.snowpark.modin.plugin\n#import os\n\n# Import Streamlit and viz modules\nimport plotly.express as px\nimport streamlit as st\nimport matplotlib\nimport plotly.io as pio\n\n# Import Snowflake modules\nimport snowflake.snowpark.functions as F\nimport snowflake.snowpark.types as T\nfrom snowflake.snowpark import Window\n\n# Import Snowflake Modeling API\nfrom snowflake.ml.modeling.pipeline import pipeline\nfrom snowflake.ml.modeling.impute import SimpleImputer\nfrom snowflake.ml.modeling.preprocessing import OneHotEncoder\nfrom snowflake.ml.modeling.linear_model import LinearRegression\nfrom snowflake.ml.modeling.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom snowflake.ml.registry import Registry\n# New\nfrom snowflake.ml.feature_store import FeatureStore, FeatureView, Entity, CreationMode\n\n\nimport warnings; warnings.simplefilter('ignore')\n\nprint(\"Imports complete.\")\n\nfrom snowflake.snowpark.context import get_active_session\nsession = get_active_session()"
  },
  {
   "cell_type": "markdown",
   "id": "23d35a7d-77a5-435b-8aab-fd452e2e5f49",
   "metadata": {
    "collapsed": false,
    "name": "Personal_Environment",
    "resultHeight": 278
   },
   "source": [
    "## Getting  your own environment\n",
    "\n",
    "So that you can mess around if you feel like it and not step on each other toes, each attendee today will get a dedicated Schema and a dedicated Virtual Warehouse.\n",
    "\n",
    "So that you can dynamically name your schema and warehouse we will use a variable as shown below.\n",
    "\n",
    "In addition we will create a [Zero Copy Clone](https://docs.snowflake.com/user-guide/tables-storage-considerations#label-cloning-tables) of the _Analytics_ schema, so that we get an instant copy of the data without incurring any delay or storage cost. Only data that we will add or modify in the base tables will result in actual extra storage.\n",
    "\n",
    "\n",
    "\n",
    "**Value**: workload isolation and instant access to full datasets for feature engineering.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d474460b-2216-41f0-a262-e9383ed6bf57",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "HOL_Environment",
    "resultHeight": 127
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"FROSTBYTE_TASTY_BYTES\"\n",
       "\"SCHEMA_MICHELLE\"\n",
       "\"MICHELLE_WH\"\n",
       "------------------------------\n",
       "Session created.\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "#Set this up to be your name as a unique ID for the lab with some python code\nMY_ID =  'MICHELLE'\nMY_WAREHOUSE = MY_ID +'_WH'\nMY_SCHEMA = 'SCHEMA_'+ MY_ID\n\n#create warehouse and use it\n#session.sql is a way to direct execute SQL on snowflake, we will focus on dataframes today\nsession.sql(\"create or replace warehouse \"+MY_WAREHOUSE+\" warehouse_type = 'SNOWPARK-OPTIMIZED' warehouse_size = 'MEDIUM' auto_suspend = 900\").collect()\nsession.use_warehouse(MY_WAREHOUSE)\n\n#create schema and use it\nsession.sql(\"create or replace schema \"+MY_SCHEMA+ \" clone analytics\" ).collect()\nsession.use_schema(MY_SCHEMA)\n\n# Add a query tag to the session. This helps with debugging and performance monitoring.\nsession.query_tag = {\"name\":\"tastybytes_hol\", \"attributes\":{\"source\":\"notebook\"}}\n\n\nprint(session.get_current_database())\nprint(session.get_current_schema())\nprint(session.get_current_warehouse())\nprint(\"------------------------------\")\nprint(\"Session created.\")\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0904d03c-a145-43ba-903b-bca2121575bc",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "sql",
    "name": "For_testing",
    "resultHeight": 112
   },
   "outputs": [],
   "source": "--USE WAREHOUSE MICHELLE_WH;\n--USE SCHEMA SCHEMA_MICHELLE;\n--SELECT CURRENT_ROLE();\n--SELECT CURRENT_WAREHOUSE();\n--SELECT CURRENT_SCHEMA();"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93b3861-ada2-42da-9acf-c44f62413ce3",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "For_testing2",
    "resultHeight": 0
   },
   "outputs": [],
   "source": [
    "#MY_SCHEMA = \"SCHEMA_MICHELLE\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3dd3e0-f22e-4ed5-bd82-69de15ad4fbb",
   "metadata": {
    "collapsed": false,
    "name": "Snowflake_Account",
    "resultHeight": 127
   },
   "source": [
    "## Snowflake Account Context\n",
    "\n",
    "\n",
    "Notice the Schema that was added under the FROSTBYTE_TASTY_BYTES Database and the Virtual Warehouse (VW) that was created. The VW started right away without the need to wait for its provisoning!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ba405a-8a5b-46e5-b6dc-357a4fdeb3f1",
   "metadata": {
    "collapsed": false,
    "name": "Part1_Prepare_Data",
    "resultHeight": 211
   },
   "source": [
    "## Snowpark DataFrame\n",
    "\n",
    "Let's create a Snowpark DataFrame containing our shift sales data from the **shift_sales** table in our Snowflake account using the Snowpark session.table function. A DataFrame is a data structure that contains rows and columns, similar to a SQL table.\n",
    "\n",
    "If you have used PySpark in the past, this should also sound very familiar - we even have [cheat sheets](https://www.snowflake.com/en/data-cloud/snowpark/spark-to-snowpark/) to help you transition quicker!\n",
    "\n",
    "\n",
    "**Value:** Familiar representation of data for Python users.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a0be32-51d6-4abf-a1ba-51e7750d349f",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "Snowpark_Dataframe",
    "resultHeight": 0
   },
   "outputs": [],
   "source": "snowpark_df = session.table(\"shift_sales\")"
  },
  {
   "cell_type": "markdown",
   "id": "a9e591f9-4b1d-43b7-ae6a-25e09d4cdf29",
   "metadata": {
    "collapsed": false,
    "name": "Lazy_Execution",
    "resultHeight": 460
   },
   "source": [
    "## What happened on Snowflake?\n",
    "\n",
    "Let's look at what was executed in Snowflake to create our location_df DataFrame. \n",
    "\n",
    "The translated SQL query can be seen in the Snowsight interface under _Activity_ in the _Query History_ \n",
    "\n",
    "We will refer to it many times over the course of this hands-on-lab so we encourage you to open it in a NEW TAB.\n",
    "\n",
    "So, what happened behind the scenes?\n",
    "\n",
    "Nothing!\n",
    "\n",
    "Why?\n",
    "\n",
    "Because Snowpark uses **Lazy execution**! \n",
    "\n",
    "Until we perform an [action](https://docs.snowflake.com/en/developer-guide/snowpark/python/working-with-dataframes#performing-an-action-to-evaluate-a-dataframe) on the dataframe like a show() or collect() type of command nothing is actually pushed down to the compute.\n",
    "\n",
    "\n",
    "\n",
    "**Value:** Efficient use of compute.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7beae5de-d798-4c78-8ef9-a1c83dc721e3",
   "metadata": {
    "collapsed": false,
    "name": "Data_Preview",
    "resultHeight": 143
   },
   "source": [
    "## Preview the Data\n",
    "\n",
    "With our Snowpark DataFrame defined, let’s use the .show() function to take a look at the first 10 rows.\n",
    "\n",
    "\n",
    "\n",
    "**Value:** Instant access to data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98d6a44-8e84-49f0-aadf-e68cc17ed5cd",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "Data_Preview_Code",
    "resultHeight": 575
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
       "|\"LOCATION_ID\"  |\"CITY\"  |\"DATE\"      |\"SHIFT_SALES\"  |\"SHIFT\"  |\"MONTH\"  |\"DAY_OF_WEEK\"  |\"LATITUDE\"  |\"LONGITUDE\"  |\"COUNT_LOCATIONS_WITHIN_HALF_MILE\"  |\"CITY_POPULATION\"  |\n",
       "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
       "|4274           |London  |2022-11-06  |NULL           |PM       |11       |0              |51.496019   |-0.177579    |6                                   |8799800            |\n",
       "|4274           |London  |2022-11-07  |NULL           |PM       |11       |1              |51.496019   |-0.177579    |6                                   |8799800            |\n",
       "|4274           |London  |2022-11-04  |NULL           |PM       |11       |5              |51.496019   |-0.177579    |6                                   |8799800            |\n",
       "|4274           |London  |2022-11-03  |NULL           |PM       |11       |4              |51.496019   |-0.177579    |6                                   |8799800            |\n",
       "|4274           |London  |2022-11-02  |NULL           |PM       |11       |3              |51.496019   |-0.177579    |6                                   |8799800            |\n",
       "|4274           |London  |2022-11-08  |NULL           |PM       |11       |2              |51.496019   |-0.177579    |6                                   |8799800            |\n",
       "|4274           |London  |2022-11-05  |NULL           |PM       |11       |6              |51.496019   |-0.177579    |6                                   |8799800            |\n",
       "|14308          |Krakow  |2022-11-06  |NULL           |PM       |11       |0              |50.057148   |19.944669    |83                                  |800653             |\n",
       "|14308          |Krakow  |2022-11-07  |NULL           |PM       |11       |1              |50.057148   |19.944669    |83                                  |800653             |\n",
       "|14308          |Krakow  |2022-11-04  |NULL           |PM       |11       |5              |50.057148   |19.944669    |83                                  |800653             |\n",
       "|14308          |Krakow  |2022-11-03  |NULL           |PM       |11       |4              |50.057148   |19.944669    |83                                  |800653             |\n",
       "|14308          |Krakow  |2022-11-02  |NULL           |PM       |11       |3              |50.057148   |19.944669    |83                                  |800653             |\n",
       "|14308          |Krakow  |2022-11-08  |NULL           |PM       |11       |2              |50.057148   |19.944669    |83                                  |800653             |\n",
       "|14308          |Krakow  |2022-11-05  |NULL           |PM       |11       |6              |50.057148   |19.944669    |83                                  |800653             |\n",
       "|6007           |Delhi   |2022-11-06  |NULL           |PM       |11       |0              |28.536786   |77.269282    |4                                   |16349831           |\n",
       "|6007           |Delhi   |2022-11-07  |NULL           |PM       |11       |1              |28.536786   |77.269282    |4                                   |16349831           |\n",
       "|6007           |Delhi   |2022-11-04  |NULL           |PM       |11       |5              |28.536786   |77.269282    |4                                   |16349831           |\n",
       "|6007           |Delhi   |2022-11-03  |NULL           |PM       |11       |4              |28.536786   |77.269282    |4                                   |16349831           |\n",
       "|6007           |Delhi   |2022-11-02  |NULL           |PM       |11       |3              |28.536786   |77.269282    |4                                   |16349831           |\n",
       "|6007           |Delhi   |2022-11-08  |NULL           |PM       |11       |2              |28.536786   |77.269282    |4                                   |16349831           |\n",
       "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
       "\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Not necessary, but will allows us to look at the queries we are generating moving forward\n",
    "query_history = session.query_history()\n",
    "\n",
    "#display some data\n",
    "snowpark_df.show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f72f9c0-4ec2-4276-8c7c-6ce4bbcd9430",
   "metadata": {
    "collapsed": false,
    "name": "SnowparkDataFrame_Transformations",
    "resultHeight": 375
   },
   "source": [
    "## Select, Filter, Sort\n",
    "\n",
    "Did you notice the Null values for \"shift_sales\"? \n",
    "\n",
    "Let's look at a single location.\n",
    "\n",
    "To do this, we will make another Snowpark DataFrame, location_df, from the above DataFrame and we will:\n",
    "\n",
    "1. Select columns\n",
    "2. Filter to a single location ID\n",
    "3. Sort by date\n",
    "\n",
    "****\n",
    "\n",
    "**Value**: Efficient transformation pipelines using Python syntax and chained logic.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef1eae4-9417-4bd8-ab6e-f8aa49375de7",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "SnowparkDataFrame_Transformations_code",
    "resultHeight": 569
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "------------------------------------------------------------------\n",
       "|\"DATE\"      |\"SHIFT\"  |\"SHIFT_SALES\"  |\"LOCATION_ID\"  |\"CITY\"   |\n",
       "------------------------------------------------------------------\n",
       "|2022-11-08  |PM       |NULL           |1637           |Toronto  |\n",
       "|2022-11-08  |AM       |NULL           |1637           |Toronto  |\n",
       "|2022-11-07  |PM       |NULL           |1637           |Toronto  |\n",
       "|2022-11-07  |AM       |NULL           |1637           |Toronto  |\n",
       "|2022-11-06  |PM       |NULL           |1637           |Toronto  |\n",
       "|2022-11-06  |AM       |NULL           |1637           |Toronto  |\n",
       "|2022-11-05  |PM       |NULL           |1637           |Toronto  |\n",
       "|2022-11-05  |AM       |NULL           |1637           |Toronto  |\n",
       "|2022-11-04  |PM       |NULL           |1637           |Toronto  |\n",
       "|2022-11-04  |AM       |NULL           |1637           |Toronto  |\n",
       "|2022-11-03  |PM       |NULL           |1637           |Toronto  |\n",
       "|2022-11-03  |AM       |NULL           |1637           |Toronto  |\n",
       "|2022-11-02  |PM       |NULL           |1637           |Toronto  |\n",
       "|2022-11-02  |AM       |NULL           |1637           |Toronto  |\n",
       "|2022-10-30  |PM       |23897.0        |1637           |Toronto  |\n",
       "|2022-10-23  |AM       |25627.0        |1637           |Toronto  |\n",
       "|2022-10-21  |AM       |22147.0        |1637           |Toronto  |\n",
       "|2022-10-16  |PM       |31417.0        |1637           |Toronto  |\n",
       "|2022-08-27  |PM       |24019.0        |1637           |Toronto  |\n",
       "|2022-08-24  |PM       |23256.0        |1637           |Toronto  |\n",
       "------------------------------------------------------------------\n",
       "\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select\n",
    "location_df = snowpark_df.select(\"date\", \"shift\", \"shift_sales\", \"location_id\", \"city\")\n",
    "\n",
    "# Filter (Vancouver=1135, Montreal=5319, Toronto = 1637)\n",
    "location_df = location_df.filter(F.col(\"location_id\") == 1135)\n",
    "#location_df = location_df.filter(F.col(\"location_id\") == 5319)\n",
    "\n",
    "# Sort\n",
    "location_df = location_df.order_by([\"date\", \"shift\"], ascending=[0, 0])\n",
    "\n",
    "# Display\n",
    "location_df.show(n=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e7ecf2-dd14-4760-a537-4f9997f6ca33",
   "metadata": {
    "collapsed": false,
    "name": "Missing_Values",
    "resultHeight": 201
   },
   "source": [
    "We can see that shift sales are populated 8 days prior to the latest date in the data. The **missing values** represent future dates that do not have shift sales yet.\n",
    "\n",
    "## Snowpark works in two main ways:\n",
    "\n",
    "1. Snowpark code translated and executed as SQL on Snowflake\n",
    "2. Python functions deployed in a secure sandbox in Snowflake\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f20ab1a-5c13-48c0-b730-2609f020ebd8",
   "metadata": {
    "codeCollapsed": true,
    "collapsed": false,
    "language": "python",
    "name": "Snowpark_diagram",
    "resultHeight": 0
   },
   "outputs": [],
   "source": [
    "st.image('https://raw.githubusercontent.com/Snowflake-Labs/sfguide-tasty-bytes-snowpark-101-for-data-science/211360e75995b670b2b25d3204b59b42e48456cd//assets/snowpark_overview.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0bc5ef9-423e-4ec4-a322-66d733a77f3e",
   "metadata": {
    "collapsed": false,
    "name": "SQL_option",
    "resultHeight": 102
   },
   "source": [
    "## SQL is also an option\n",
    "\n",
    "Please also note that if at any point there is some code that you'd rather write in SQL, you are just a click away from adding a _SQL Cell_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ccbe5a-d86d-4b1f-a53e-92bc413e9f8a",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "sql",
    "name": "SQL",
    "resultHeight": 439
   },
   "outputs": [],
   "source": [
    "SELECT \"DATE\", \"SHIFT\", \"SHIFT_SALES\", \"LOCATION_ID\", \"CITY\" \n",
    "FROM frostbyte_tasty_bytes.analytics.shift_sales \n",
    "WHERE (\"LOCATION_ID\" = 1637) ORDER BY \"DATE\" DESC, \"SHIFT\" DESC  \n",
    "LIMIT 20;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917213a6-707f-42de-9f6a-05d0c463ee8a",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "session_SQL",
    "resultHeight": 329
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "------------------------------------------------------------------\n",
       "|\"DATE\"      |\"SHIFT\"  |\"SHIFT_SALES\"  |\"LOCATION_ID\"  |\"CITY\"   |\n",
       "------------------------------------------------------------------\n",
       "|2022-11-08  |PM       |NULL           |1637           |Toronto  |\n",
       "|2022-11-08  |AM       |NULL           |1637           |Toronto  |\n",
       "|2022-11-07  |PM       |NULL           |1637           |Toronto  |\n",
       "|2022-11-07  |AM       |NULL           |1637           |Toronto  |\n",
       "|2022-11-06  |PM       |NULL           |1637           |Toronto  |\n",
       "|2022-11-06  |AM       |NULL           |1637           |Toronto  |\n",
       "|2022-11-05  |PM       |NULL           |1637           |Toronto  |\n",
       "|2022-11-05  |AM       |NULL           |1637           |Toronto  |\n",
       "|2022-11-04  |PM       |NULL           |1637           |Toronto  |\n",
       "|2022-11-04  |AM       |NULL           |1637           |Toronto  |\n",
       "------------------------------------------------------------------\n",
       "\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.sql(\"SELECT DATE, SHIFT, SHIFT_SALES, LOCATION_ID, CITY \\\n",
    "FROM frostbyte_tasty_bytes.analytics.shift_sales \\\n",
    "WHERE (LOCATION_ID = 1637) ORDER BY DATE DESC, SHIFT DESC \\\n",
    "LIMIT 20\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc96f3d-27eb-43ef-8f6d-652650922e76",
   "metadata": {
    "collapsed": false,
    "name": "Snowpark_Query_History",
    "resultHeight": 211
   },
   "source": [
    "## Explain the Query\n",
    "\n",
    "Everything  you do in Snowflake UI can be done in code as well. Checking the latest query is no different.\n",
    "\n",
    "Which means you don't actually need to use the Snowsight UI to see the translated SQL. You can simply query the latest record in the Query History view.\n",
    "\n",
    "**Value:** Transparent execution and compute usage.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de22c7d7-9212-40c3-af84-ea2dea562666",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "Query_History_code",
    "resultHeight": 308
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[QueryRecord(query_id='01b3c0dc-3201-3053-0002-c96200e0e3ce', sql_text='SELECT  *  FROM (SELECT DATE, SHIFT, SHIFT_SALES, LOCATION_ID, CITY FROM frostbyte_tasty_bytes.analytics.shift_sales WHERE (LOCATION_ID = 1637) ORDER BY DATE DESC, SHIFT DESC LIMIT 20) LIMIT 10')]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the SQL that just executed on Snowflake!\n",
    "query_history.queries[-1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fcbc739-f32b-45b9-97a3-8b3d4a45bcef",
   "metadata": {
    "collapsed": false,
    "name": "Query_Explain",
    "resultHeight": 150
   },
   "source": [
    "To be fair you don't even need to run the query to understand what is going to happen! \n",
    "\n",
    "You can use the `explain()` function to get the dataframe execution plan and ensure it looks efficient and that Snowflake will be able to prune as much data as possible.\n",
    "\n",
    "\n",
    "\n",
    "**Value:** Transparent execution and compute usage.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3039381d-a8cc-40a6-99d2-37df62c72d04",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "Query_Explain_code",
    "resultHeight": 0
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "---------DATAFRAME EXECUTION PLAN----------\n",
       "Query List:\n",
       "1.\n",
       "SELECT \"DATE\", \"SHIFT\", \"SHIFT_SALES\", \"LOCATION_ID\", \"CITY\" FROM shift_sales WHERE (\"LOCATION_ID\" = 1637 :: INT) ORDER BY \"DATE\" DESC NULLS LAST, \"SHIFT\" DESC NULLS LAST\n",
       "Logical Execution Plan:\n",
       "GlobalStats:\n",
       "    partitionsTotal=512\n",
       "    partitionsAssigned=491\n",
       "    bytesAssigned=10372608\n",
       "Operations:\n",
       "1:0     ->Result  SHIFT_SALES.DATE, SHIFT_SALES.SHIFT, SHIFT_SALES.SHIFT_SALES, SHIFT_SALES.LOCATION_ID, SHIFT_SALES.CITY  \n",
       "1:1          ->Sort  SHIFT_SALES.DATE DESC NULLS LAST, SHIFT_SALES.SHIFT DESC NULLS LAST  \n",
       "1:2               ->Filter  SHIFT_SALES.LOCATION_ID = 1637  \n",
       "1:3                    ->TableScan  FROSTBYTE_TASTY_BYTES.SCHEMA_MICHELLE.SHIFT_SALES  LOCATION_ID, CITY, DATE, SHIFT_SALES, SHIFT  {partitionsTotal=512, partitionsAssigned=491, bytesAssigned=10372608}\n",
       "\n",
       "--------------------------------------------\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "location_df.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a05d623-7d66-464c-9095-bdc68ed6659b",
   "metadata": {
    "collapsed": false,
    "name": "CompareSFDataFramevsPandasDataFrame",
    "resultHeight": 221
   },
   "source": [
    "## Compare DataFrame Size\n",
    "\n",
    "Let's bring a sample of our Snowflake dataset to our local environment (AKA our browser) in a pandas DataFrame using the `to_pandas()` function. We will compare how much memory is used for the pandas DataFrame compared to the Snowpark DataFrame. As we will see, no memory is used for the Snowpark DataFrame in our Python environment. All data in the Snowpark DataFrame remains on Snowflake.\n",
    "\n",
    "\n",
    "**Value:** No copies or movement of data when working with Snowpark DataFrames.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17cffefc-918b-4e14-a05a-3526feea8892",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "CompareSFDataFramevsPandasDataFrame_code",
    "resultHeight": 0
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Snowpark DataFrame Size (snowpark_df): 0.00 MB\n",
       "Pandas DataFrame Size (pandas_df): 0.19 MB\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bring 10,000 rows from Snowflake to pandas\n",
    "pandas_df = snowpark_df.limit(10000).to_pandas()\n",
    "\n",
    "# Get Snowpark DataFrame size\n",
    "snowpark_size = sys.getsizeof(snowpark_df) / (1024*1024)\n",
    "print(f\"Snowpark DataFrame Size (snowpark_df): {snowpark_size:.2f} MB\")\n",
    "\n",
    "# Get pandas DataFrame size\n",
    "pandas_size = sys.getsizeof(pandas_df) / (1024*1024)\n",
    "print(f\"Pandas DataFrame Size (pandas_df): {pandas_size:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4f0bb6-cff6-46ed-8f58-b5666c1fcc36",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "cell3",
    "resultHeight": 0
   },
   "outputs": [],
   "source": [
    "st.image('https://raw.githubusercontent.com/Snowflake-Labs/sfguide-tasty-bytes-snowpark-101-for-data-science/211360e75995b670b2b25d3204b59b42e48456cd//assets/data_exploration.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d83d654-37fe-479a-9141-c5e4a8277221",
   "metadata": {
    "collapsed": false,
    "name": "Data_Exploration",
    "resultHeight": 150
   },
   "source": [
    "Here we will use Snowpark to explore our data. A common pattern for exploration is to use Snowpark to manipulate our data and then bring an aggregate table to our Python environment for visualization.\n",
    "\n",
    "**Value:**\n",
    "- Native Snowflake performance and scale for aggregating large datasets.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ec8211-bf96-4aa1-a195-9cb4743e6d16",
   "metadata": {
    "collapsed": false,
    "name": "SnowparkDF_count",
    "resultHeight": 169
   },
   "source": [
    "## How many rows are in our data?\n",
    "\n",
    "This will give us an idea of how we might need to approach working with this data. Do we have enough data to build a meaningful model? What compute might be required? Will we need to sample the data?\n",
    "\n",
    "**What's happening where?:** Rows counted in Snowflake. No data transfer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833534d4-834b-4c07-9bb7-fe31948e6bc6",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "SnowparkDF_count_code",
    "resultHeight": 0
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "707540"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snowpark_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e903e3ed-df02-437f-a5b1-a31646439a5d",
   "metadata": {
    "collapsed": false,
    "name": "Descriptive_Stats",
    "resultHeight": 195
   },
   "source": [
    "## Let's calculate some descriptive statistics.\n",
    "\n",
    "We use the Snowpark `describe()` function to calculate summary statistics and then bring the aggregate results into a pandas DataFrame to visualize in a formatted table.\n",
    "\n",
    "**What's happening where?:** Summary statistics calculated in Snowflake. Transfer aggregate summary statistics for client-side visualization.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3178f289-b6fb-4144-b2ea-3de3c39473f1",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "Descriptive_Stats_code",
    "resultHeight": 0
   },
   "outputs": [
    {
     "data": {
      "application/vnd.hex.export+parquet": "{\"success\":true,\"exportKey\":\"snowpark_canada/b215be44-db76-4c2d-a4cf-d34051f64ca7/exports/760fe09f-ccb2-4c38-9706-46035fe4e07e\"}",
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SUMMARY</th>\n",
       "      <th>LOCATION_ID</th>\n",
       "      <th>CITY</th>\n",
       "      <th>SHIFT_SALES</th>\n",
       "      <th>SHIFT</th>\n",
       "      <th>MONTH</th>\n",
       "      <th>DAY_OF_WEEK</th>\n",
       "      <th>LATITUDE</th>\n",
       "      <th>LONGITUDE</th>\n",
       "      <th>COUNT_LOCATIONS_WITHIN_HALF_MILE</th>\n",
       "      <th>CITY_POPULATION</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>count</td>\n",
       "      <td>707540.000000</td>\n",
       "      <td>707540</td>\n",
       "      <td>524420.000000</td>\n",
       "      <td>707540</td>\n",
       "      <td>707540.000000</td>\n",
       "      <td>707540.000000</td>\n",
       "      <td>707540.000000</td>\n",
       "      <td>707540.000000</td>\n",
       "      <td>707540.000000</td>\n",
       "      <td>7.075400e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>stddev</td>\n",
       "      <td>4199.251255</td>\n",
       "      <td>None</td>\n",
       "      <td>12077.623062</td>\n",
       "      <td>None</td>\n",
       "      <td>3.415971</td>\n",
       "      <td>2.001384</td>\n",
       "      <td>28.384873</td>\n",
       "      <td>77.663891</td>\n",
       "      <td>51.598726</td>\n",
       "      <td>4.529228e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>max</td>\n",
       "      <td>15517.000000</td>\n",
       "      <td>Warsaw</td>\n",
       "      <td>78079.000000</td>\n",
       "      <td>PM</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>59.486683</td>\n",
       "      <td>151.323435</td>\n",
       "      <td>290.000000</td>\n",
       "      <td>1.634983e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mean</td>\n",
       "      <td>8195.895190</td>\n",
       "      <td>None</td>\n",
       "      <td>19280.000420</td>\n",
       "      <td>None</td>\n",
       "      <td>7.649166</td>\n",
       "      <td>2.998681</td>\n",
       "      <td>31.831126</td>\n",
       "      <td>-2.486710</td>\n",
       "      <td>29.874000</td>\n",
       "      <td>4.281827e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>min</td>\n",
       "      <td>1001.000000</td>\n",
       "      <td>Barcelona</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>AM</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-38.327454</td>\n",
       "      <td>-123.243134</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.056610e+05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snowpark_df.describe().to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0694cf3-6dd8-46d6-bac9-0c5aa9c962d2",
   "metadata": {
    "collapsed": false,
    "name": "Numeric_Columns",
    "resultHeight": 169
   },
   "source": [
    "## What are the numeric columns?\n",
    "\n",
    "We want to understand the data types in our data and how we might need to handle them in preparation for modeling. For numeric columns, this could include normalizing the data to the same scale or applying a transformation to change the distribution.\n",
    "\n",
    "**What's happening where?:** The Snowflake table schema is used to get metadata information about the data. No data transfer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372d88e6-a69c-4b78-be72-8611479fb485",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "Numeric_Columns_code",
    "resultHeight": 0
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LOCATION_ID',\n",
       " 'SHIFT_SALES',\n",
       " 'MONTH',\n",
       " 'DAY_OF_WEEK',\n",
       " 'LATITUDE',\n",
       " 'LONGITUDE',\n",
       " 'COUNT_LOCATIONS_WITHIN_HALF_MILE',\n",
       " 'CITY_POPULATION']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define Snowflake numeric types\n",
    "numeric_types = [T.DecimalType, T.DoubleType, T.FloatType, T.IntegerType, T.LongType]\n",
    "\n",
    "# Get numeric columns\n",
    "numeric_columns = [col.name for col in snowpark_df.schema.fields if type(col.datatype) in numeric_types]\n",
    "numeric_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bae642f-a7ce-4d99-8d57-4a8f6b6e73b4",
   "metadata": {
    "collapsed": false,
    "name": "Categorical_Columns",
    "resultHeight": 169
   },
   "source": [
    "## What are the categorical columns?\n",
    "\n",
    "Our model requires all features to be numeric. We want to identify columns that we will need to transform to a numeric representation if we would like to use them as features in our model.\n",
    "\n",
    "**What's happening where?:** The Snowflake table schema is used to get metadata information about the data. No data transfer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415a134b-5d22-41f6-a1ee-f8be0a2effcd",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "Categorical_Columns_code",
    "resultHeight": 0
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CITY', 'SHIFT']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "# Define Snowflake categorical types\ncategorical_types = [T.StringType]\n\n# Get categorical columns\ncategorical_columns = [col.name for col in snowpark_df.schema.fields if type(col.datatype) in categorical_types]\ncategorical_columns"
  },
  {
   "cell_type": "markdown",
   "id": "b96e7f8b-ed53-43d8-9f4c-ad638a0953c6",
   "metadata": {
    "collapsed": false,
    "name": "Average_Shift_Sales",
    "resultHeight": 195
   },
   "source": [
    "## What are the average shift sales (USD) by city?\n",
    "\n",
    "Here, we are trying to understand what a \"normal\" shift sale looks like. What is the span of averages across cities? Are there any outlier cities that should be removed from our training data? Is there anything unexpected in the order of cities sorted by their average shift sales?\n",
    "\n",
    "**What's happening where?:** Average sales by city calculated in Snowflake. Transfer city averages for client-side visualization.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da0104e-19bb-4ca2-93d8-8f56659e2af3",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "Average_Shift_Sale_code",
    "resultHeight": 0
   },
   "outputs": [],
   "source": [
    "# Group by city and average shift sales\n",
    "analysis_df = snowpark_df.group_by(\"city\").agg(F.mean(\"shift_sales\").alias(\"avg_shift_sales\"))\n",
    "\n",
    "# Sort by average shift sales\n",
    "analysis_df = analysis_df.sort(\"avg_shift_sales\", ascending=True)\n",
    "\n",
    "# Pull to pandas and plot\n",
    "analysis_df_pandas = analysis_df.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc679798-60c6-4095-bf8d-38f5014c6195",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "Average_Shift_Sales_Bar_Chart",
    "resultHeight": 0
   },
   "outputs": [],
   "source": "fig = px.bar(analysis_df_pandas, x='CITY',y='AVG_SHIFT_SALES',\n      title=\"Average Shift Sales by City\")\nfig.update_traces(textfont_size=12, textangle=0, textposition=\"outside\", cliponaxis=False)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5b77a9-e668-4c2b-a28e-3a49b95b140d",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "City_Selection",
    "resultHeight": 0
   },
   "outputs": [],
   "source": [
    "st.title(\"Select a city for further analysis\")\n",
    "\n",
    "selected_city = 'Toronto'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562b56b2-e8ed-4019-8a31-00ea4dc48622",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "Selected_City_as_Variable",
    "resultHeight": 0
   },
   "outputs": [],
   "source": [
    "st.markdown(f\"## Looking at the {selected_city}, how many locations are there?\")\n",
    "\n",
    "st.markdown(\"Let's get to know locations and shift sales in that city. First, we will see how many location options there are in the selected city for a food truck to park.\")\n",
    "\n",
    "st.markdown(\"**What's happening where?:** Data filtered, averages calculated by location, and locations counted in Snowflake. No data transfer.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a520a6-8e42-47a2-9f5d-7a07e96ef57e",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "Selected_City_code",
    "resultHeight": 0
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "San Mateo location count: 372\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter to selected_city\n",
    "analysis_df = snowpark_df.filter(F.col(\"city\") == selected_city)\n",
    "\n",
    "# Group by location and average shift sales\n",
    "analysis_df = analysis_df.group_by(\"location_id\").agg(F.mean(\"shift_sales\").alias(\"avg_shift_sales\"))\n",
    "\n",
    "print(selected_city,\":\", analysis_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc64c58b-26eb-46ca-b868-92b2539b761a",
   "metadata": {
    "codeCollapsed": true,
    "collapsed": false,
    "language": "python",
    "name": "Feature_Engineering_banner",
    "resultHeight": 0
   },
   "outputs": [],
   "source": [
    "st.image('https://raw.githubusercontent.com/Snowflake-Labs/sfguide-tasty-bytes-snowpark-101-for-data-science/211360e75995b670b2b25d3204b59b42e48456cd//assets/feature_engineering.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c568877d-0d6d-48f8-8332-107f34d0f917",
   "metadata": {
    "collapsed": false,
    "name": "Feature_Engineering",
    "resultHeight": 439
   },
   "source": "## Feature Engineering\n\nNow let's keep revelant columns and transform columns to create features needed for our prediction model. We're going to store these features in a Feature Store to be used for model training and model inference. Feature Stores are great for ensuring feature reusability, standard feature definitions and ensuring consistency in model training and serving.\n\n****\n\n## Create a Rolling Average Feature\n\nWe will use a Snowflake window function to get a **rolling shift average by location** over time. Window functions allow us to aggregate on a \"moving\" group of rows.\n\n**Step 1. Create a Window**\n\nOur window will partition the data by location and shift. It will order rows by date. It will include all rows prior to the current date of the observation it is aggregating for.\n\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483ab7cb-0f96-4f67-a029-c9da2c528221",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "Create_Window",
    "resultHeight": 0
   },
   "outputs": [],
   "source": [
    "window_by_location_all_days = (\n",
    "    Window.partition_by(\"location_id\", \"shift\")\n",
    "    .order_by(\"date\")\n",
    "    .rows_between(Window.UNBOUNDED_PRECEDING, Window.CURRENT_ROW - 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbf4ff0-26dc-4eec-b376-96f7e8143395",
   "metadata": {
    "collapsed": false,
    "name": "Rolling_Average",
    "resultHeight": 41
   },
   "source": [
    "**Step 2. Aggregate across the Window**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5edbdfc4-4a16-431e-9703-eced3c58c0b4",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "Rolling_Average_code",
    "resultHeight": 345
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
       "|\"LOCATION_ID\"  |\"CITY\"  |\"DATE\"      |\"SHIFT_SALES\"  |\"SHIFT\"  |\"MONTH\"  |\"DAY_OF_WEEK\"  |\"LATITUDE\"  |\"LONGITUDE\"  |\"COUNT_LOCATIONS_WITHIN_HALF_MILE\"  |\"CITY_POPULATION\"  |\"AVG_LOCATION_SHIFT_SALES\"  |\n",
       "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
       "|7705           |Sydney  |2020-05-27  |29446.0        |PM       |5        |3              |-33.86184   |151.20834    |111                                 |5231147            |NULL                        |\n",
       "|7705           |Sydney  |2020-06-15  |27165.0        |PM       |6        |1              |-33.86184   |151.20834    |111                                 |5231147            |29446.0                     |\n",
       "|7705           |Sydney  |2021-04-12  |42279.0        |PM       |4        |1              |-33.86184   |151.20834    |111                                 |5231147            |28305.5                     |\n",
       "|7705           |Sydney  |2021-06-24  |28219.0        |PM       |6        |4              |-33.86184   |151.20834    |111                                 |5231147            |32963.333333333336          |\n",
       "|7705           |Sydney  |2021-08-16  |27182.0        |PM       |8        |1              |-33.86184   |151.20834    |111                                 |5231147            |31777.25                    |\n",
       "|7705           |Sydney  |2021-08-29  |29430.0        |PM       |8        |0              |-33.86184   |151.20834    |111                                 |5231147            |30858.2                     |\n",
       "|7705           |Sydney  |2021-10-14  |32950.0        |PM       |10       |4              |-33.86184   |151.20834    |111                                 |5231147            |30620.166666666668          |\n",
       "|7705           |Sydney  |2021-11-30  |25361.0        |PM       |11       |2              |-33.86184   |151.20834    |111                                 |5231147            |30953.0                     |\n",
       "|7705           |Sydney  |2022-04-06  |34762.0        |PM       |4        |3              |-33.86184   |151.20834    |111                                 |5231147            |30254.0                     |\n",
       "|7705           |Sydney  |2022-05-01  |33272.0        |PM       |5        |0              |-33.86184   |151.20834    |111                                 |5231147            |30754.88888888889           |\n",
       "|7705           |Sydney  |2022-10-30  |48491.75       |PM       |10       |0              |-33.86184   |151.20834    |111                                 |5231147            |31006.6                     |\n",
       "|7705           |Sydney  |2022-11-02  |NULL           |PM       |11       |3              |-33.86184   |151.20834    |111                                 |5231147            |32596.159090909092          |\n",
       "|7705           |Sydney  |2022-11-03  |NULL           |PM       |11       |4              |-33.86184   |151.20834    |111                                 |5231147            |32596.159090909092          |\n",
       "|7705           |Sydney  |2022-11-04  |NULL           |PM       |11       |5              |-33.86184   |151.20834    |111                                 |5231147            |32596.159090909092          |\n",
       "|7705           |Sydney  |2022-11-05  |NULL           |PM       |11       |6              |-33.86184   |151.20834    |111                                 |5231147            |32596.159090909092          |\n",
       "|7705           |Sydney  |2022-11-06  |NULL           |PM       |11       |0              |-33.86184   |151.20834    |111                                 |5231147            |32596.159090909092          |\n",
       "|7705           |Sydney  |2022-11-07  |NULL           |PM       |11       |1              |-33.86184   |151.20834    |111                                 |5231147            |32596.159090909092          |\n",
       "|7705           |Sydney  |2022-11-08  |NULL           |PM       |11       |2              |-33.86184   |151.20834    |111                                 |5231147            |32596.159090909092          |\n",
       "|6507           |Cairo   |2020-06-06  |10898.0        |PM       |6        |6              |29.996784   |30.968879    |0                                   |1010166            |NULL                        |\n",
       "|6507           |Cairo   |2020-07-08  |16140.0        |PM       |7        |3              |29.996784   |30.968879    |0                                   |1010166            |10898.0                     |\n",
       "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
       "\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "snowpark_df_features = snowpark_df.with_column(\n    \"avg_location_shift_sales\", \n    F.avg(\"shift_sales\").over(window_by_location_all_days)\n)\n\nsnowpark_df_features = snowpark_df_features.select(\n        \"location_id\", \"date\", \"shift\", \"avg_location_shift_sales\"\n)\n\nsnowpark_df_features.show(n=10)"
  },
  {
   "cell_type": "markdown",
   "id": "0e0cce0b-12f6-4853-b092-e699968c7cc7",
   "metadata": {
    "name": "Feature_Store_intro",
    "collapsed": false,
    "resultHeight": 431
   },
   "source": "In the **[Snowflake Feature Store](https://docs.snowflake.com/en/developer-guide/snowpark-ml/feature-store/overview)**, as typical of other Feature Store solutions:\n\n- **Entities** - define the business-entity and the level that we want to gather data and develop ML models at. (e.g. store or/and product key etc).\n- **Features** are defined and grouped within **FeatureViews**. In Snowflake Feature Store features are columns, or column-expressions defined via the Snowpark for Python dataframe api, or via SQL directly.\n- **FeatureViews** are associated (defined) for one or more **Entities**. A **FeatureView** can be defined with 1:n Entities, but typically only one. Several (many) **FeatureViews** may contain Features for the same Entity. FeatureViews tend to get defined based on the data-source they are derived from, the data's refresh or calculation frequency. A **FeatureView** is defined via a Snowpark Dataframe (or SQL expression) enabling a complex pipeline to be used.\n- The **Entity** (key columns) are used to join **FeatureViews** together when needed to gather features from multiple **FeatureViews** within a single training or inference dataset, or derive new **FeatureViews**.\n- A **FeatureSlice** provides a way of creating a subset of the Features from a single **FeatureViews** when needed. It can be used within the API, pretty much anywhere the **FeatureViews** can be used.\n- **FeatureViews** and **FeatureSlices** can be merged (via merge_features) to gather features together and create a new **FeatureView**. For example, all the features for a given **Entity** could be gathered via the merge into a single."
  },
  {
   "cell_type": "markdown",
   "id": "45edf147-e5a7-4557-8478-44de5f3aaad7",
   "metadata": {
    "name": "Registering_Feature_Store",
    "collapsed": false,
    "resultHeight": 289
   },
   "source": "### Creating and Registering the Feature Store and Entity\nBefore we can calculate and register our features, we need to create a feature store and define an entity that encapsulates the keys used to join features.\n- **Feature Store Creation:** The feature store is created within a specified database and schema, and it is configured to create if it does not already exist.\n- **Entity Definition:** An entity named AGGREGATE_WINDOW is defined, which includes the join keys location_id and shift that will be used for feature lookups to generate datasets.\n- **Entity Registration:** The entity is registered in the feature store if it doesn't already exist. If it does exist, the existing entity is retrieved."
  },
  {
   "cell_type": "code",
   "id": "cca34443-22a5-4c23-8b24-2e7d55f792df",
   "metadata": {
    "language": "python",
    "name": "Feature_Store",
    "collapsed": false,
    "resultHeight": 38
   },
   "outputs": [],
   "source": "# Import necessary libraries and create session\nfrom snowflake.ml.feature_store import FeatureStore, CreationMode\n\n# Create a Feature Store\nfs = FeatureStore(\n    session=session,\n    database=session.get_current_database(),\n    name=session.get_current_schema(),\n    default_warehouse=session.get_current_warehouse(),\n    creation_mode=CreationMode.CREATE_IF_NOT_EXIST,\n)\n\nprint(\"FeatureStore created or referenced successfully.\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "822599ae-6f1e-4d4d-916b-84b57779a748",
   "metadata": {
    "language": "python",
    "name": "Entity_Definition",
    "collapsed": false,
    "resultHeight": 143
   },
   "outputs": [],
   "source": "# Define the name and keys for the AGGREGATE_WINDOW entity\naggregate_window_entity_name = \"AGGREGATE_WINDOW\"\naggregate_window_entity_join_keys = [\"location_id\", \"shift\"]\n\n# List existing entities in the feature store\nexisting_entities = json.loads(fs.list_entities().select(F.to_json(F.array_agg(\"NAME\", True))).collect()[0][0])\n\n# Check if the AGGREGATE_WINDOW entity already exists\nif aggregate_window_entity_name not in existing_entities:\n    # Create the AGGREGATE_WINDOW entity\n    aggregate_window_entity = Entity(\n        name=aggregate_window_entity_name, \n        join_keys=aggregate_window_entity_join_keys, \n        desc=\"Aggregate window for rolling shift average by location\"\n    )\n    fs.register_entity(aggregate_window_entity)\nelse:\n    # Get the existing AGGREGATE_WINDOW entity\n    aggregate_window_entity = fs.get_entity(aggregate_window_entity_name)\n\n# Show our newly created entity\n# snowpark.DataFrame.show() is another way to preview the DataFrame contents\nfs.list_entities().show()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8ee12552-f884-4e8a-842f-7ad012f38382",
   "metadata": {
    "name": "Feature_View",
    "collapsed": false,
    "resultHeight": 206
   },
   "source": "### Create a new FeatureView and materialize the feature pipeline\nFeature Views contains all the features you need for your model training and inference.\n\nNow we construct a Feature View with above snowpark_df_features DataFrame. First, we create a draft feature view (fv). We set the refresh_freq to 30 days which will create a Snowflake dynamic table . At this point, the draft feature view will not take effect because it is not registered yet. Then we register the feature view by via register_feature_view. It will materialize to Snowflake backend. Incremental maintenance will start if the query is supported."
  },
  {
   "cell_type": "code",
   "id": "5b2e445b-e18c-4f6a-8046-3c0699e5217c",
   "metadata": {
    "language": "python",
    "name": "Feature_View_code",
    "collapsed": false,
    "resultHeight": 0
   },
   "outputs": [],
   "source": "from snowflake.ml.feature_store import FeatureView\nfrom snowflake.snowpark import Window\nimport snowflake.snowpark.functions as F\n\n# Define the feature view\nfeature_view = FeatureView(\n    name=\"AGGREGATE_WINDOW_FV\",\n    entities=[aggregate_window_entity],   # Use the entity created above\n    feature_df=snowpark_df_features,      # This is your Snowpark DataFrame with features\n    timestamp_col=\"DATE\",                 # Use your timestamp column here\n    refresh_freq=\"30 days\",               # Set the refresh frequency according to your needs\n    desc=\"TastyBytes rolling shift average by location_id refreshed on a schedule\"\n)\n\n# Register the feature view\nregistered_fv = fs.register_feature_view(\n    feature_view=feature_view,\n    version=\"1\",\n    block=True\n)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4445138f-c34d-469d-868a-fabe03113c0d",
   "metadata": {
    "language": "python",
    "name": "Get_feature_view",
    "collapsed": false,
    "resultHeight": 0
   },
   "outputs": [],
   "source": "AGGREGATE_WINDOW_FV = fs.get_feature_view(\n    name=\"AGGREGATE_WINDOW_FV\", \n    version='1')",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3fd0b855-247d-4515-98ea-7e13827f7433",
   "metadata": {
    "name": "Generate_Training_data",
    "collapsed": false,
    "resultHeight": 153
   },
   "source": "## Generate training data\nWe can generate training data easily from Feature Store and output it either as a Dataset object, or as Snowpark DataFrame. The cell below creates a spine dataframe by randomly sampling some entity keys from source table. generate_dataset() then creates a Dataset object by populating the spine_df with respective feature values from selected feature views."
  },
  {
   "cell_type": "code",
   "id": "4dac694f-8972-4e39-8248-618eb2be5e1c",
   "metadata": {
    "language": "python",
    "name": "Generate_training_data_code",
    "collapsed": false,
    "resultHeight": 351
   },
   "outputs": [],
   "source": "data_features = fs.generate_training_set(\n    spine_df=snowpark_df,\n    features=[AGGREGATE_WINDOW_FV],\n    spine_label_cols=[\"CITY\",\"CITY_POPULATION\",\"LATITUDE\",\"LONGITUDE\",\"SHIFT_SALES\"],      # optional\n)\n\n# Show preview of the Dataset contents =\ndata_features.show(10)\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "bf1476c8-1bb7-4a43-8f84-62c7fb37e709",
   "metadata": {
    "name": "Pipeline",
    "resultHeight": 303,
    "collapsed": false
   },
   "source": "## Building a Pipeline\n\nTo make some of our features more useful, we will normalize them using standard preprocessing techniques, such as One-Hot Encoding. Let's fit an encoder to our data, then use it to transform the data, producing new feature columns.\n\n**Value:** The Snowpark syntax makes pipelines easy to implement and understand. The syntax also allows for easy migration of Spark pipelines to Snowflake.\n\nWith SnowparkML, you can use a standard sklearn-style API to execute **fully distributed feature engineering preprocessing tasks on Snowflake compute, with zero data movement.**\n\n**Value**: Near-zero maintenance. Focus on the work that brings value.\n"
  },
  {
   "cell_type": "markdown",
   "id": "cb1c3b9a-e98c-4a7e-ae07-a8cf95b7d2e1",
   "metadata": {
    "collapsed": false,
    "name": "Impute_Missing_Values",
    "resultHeight": 127
   },
   "source": [
    "## Impute Missing Values with Snowpark ML\n",
    "\n",
    "\n",
    "\n",
    "The rolling average feature we just created is missing if there are no prior shift sales at that location. We will replace those missing values with 0.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3b971a-b7aa-4cc0-b8f2-4a1447511d04",
   "metadata": {
    "codeCollapsed": true,
    "collapsed": false,
    "language": "python",
    "name": "Encode",
    "resultHeight": 713
   },
   "outputs": [],
   "source": [
    "st.markdown(\"**One Hot Encoding with Snowpark ML**\")\n",
    "st.image('https://miro.medium.com/v2/resize:fit:1400/format:webp/0*WcPabPyXynVjGcBP')\n",
    "\n",
    "st.markdown(f\"Categorical columns need to be represented as numeric in our model. We will use the OneHotEncoder method from the PreProcessing module to encode the columh 'Shift'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef48ef4-77ce-409c-9ec8-a324f0b563af",
   "metadata": {
    "codeCollapsed": true,
    "collapsed": false,
    "language": "python",
    "name": "Model_training_1",
    "resultHeight": 601
   },
   "outputs": [],
   "source": [
    "st.image('https://raw.githubusercontent.com/Snowflake-Labs/sfguide-tasty-bytes-snowpark-101-for-data-science/211360e75995b670b2b25d3204b59b42e48456cd//assets/model_training.png')\n",
    "st.markdown(\"**Option 1 for Model Training**\");\n",
    "st.markdown(f\"We will now use our training data to train a linear regression model on Snowflake.\")\n",
    "st.markdown(f\"We will be leveraging the deployment of Python functions into Snowflake for training and model deployment\")\n",
    "st.image('https://raw.githubusercontent.com/Snowflake-Labs/sfguide-tasty-bytes-snowpark-101-for-data-science/211360e75995b670b2b25d3204b59b42e48456cd//assets/end_to_end_ml.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7352562c-9d98-42a9-8ae4-12f7226266d7",
   "metadata": {
    "collapsed": false,
    "name": "Model_Training",
    "resultHeight": 294
   },
   "source": [
    "\n",
    "Here, we see a typical data science workflow. We are finished **preparing** our data and now move on to **training in a Python stored procedure on Snowflake**. The model created from this stored procedure will be our tool for automating decisions around truck locations to maximize our revenue. We'll surface the predicted sales (model inference) on future data using a **Python user-defined function** to drive the decisions.\n",
    "\n",
    "**Snowflake Stored Procedures** work well for training because they can read data, hold an entire table in memory to find patterns, and write files (e.g. model files) back to the Snowflake database.\n",
    "\n",
    "**Snowflake User-Defined Functions** work well for inference because they return a single value for each row passed to the user-defined function. Because of this, they can easily be distributed to provide fast results.\n",
    "\n",
    "**Value**: Effortless, scalable, and secure processing **without data movement** across compute environments.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fcba97-e7c3-4f1d-b356-c0f5d8e0db77",
   "metadata": {
    "codeCollapsed": true,
    "collapsed": false,
    "language": "python",
    "name": "End_to_End",
    "resultHeight": 562
   },
   "outputs": [],
   "source": [
    "st.markdown(\"**Simplify, accelerate and scale end-to-end AI/ML workflows**\")\n",
    "st.image(\"https://www.snowflake.com/wp-content/uploads/2023/06/Screenshot-2023-06-27-at-7.53.44-PM.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c92592-13de-433d-a3d7-528778562f33",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "Scale_Up",
    "resultHeight": 0
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(status='Statement executed successfully.')]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.sql(\"ALTER WAREHOUSE \" + MY_WAREHOUSE + \" SET WAREHOUSE_SIZE = 'LARGE'\").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4523ce-53da-4941-9eb6-5f05b9da4f13",
   "metadata": {
    "collapsed": false,
    "name": "ML_Modeling_API",
    "resultHeight": 356
   },
   "source": "## Option 2:  Model Training using ML Modeling API\n\n\nLet's run our Linear Regression training job using the SnowparkML Modeling API- this will push down our model training to run on Snowflake.\n\n- `Pipeline` allows you to sequentially apply a list of transforms.\n- `model.fit()` function actually creates a temporary stored procedure in the background. This also means that the model training is a single-node operation but will use multiple cores within the node via the n_jobs argument if used. Be sure to use a Snowpark Optimized Warehouse if you need more memory.\n- `model.predict()` function actually creates a temporary vectorized UDF in the background, which means the input DataFrame is batched as Pandas DataFrames and inference is parallelized across the batches of data. You can check the query history once you execute the following cell to check. The result is a dataframe with the results appended to it, not just the results like vanilla scikit-learn. \n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26281f49-6d47-48dc-a2fb-55421a7decbf",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "ML_Modeling_API_code",
    "resultHeight": 0
   },
   "outputs": [],
   "source": "## linregression \nfrom snowflake.ml.modeling.pipeline import Pipeline\nfrom snowflake.ml.modeling.impute import SimpleImputer\nfrom snowflake.ml.modeling.preprocessing import OneHotEncoder\nfrom snowflake.ml.modeling.linear_model import LinearRegression\nfrom snowflake.ml.modeling.metrics import mean_squared_error, mean_absolute_error, r2_score\n\n#remove date column because the date datatype is causing pipeline errors\ndata_features = data_features.drop(\"date\")\n\ntrain_snowpark_df, test_snowpark_df = data_features.randomSplit([0.8, 0.2])\n# Save training data\ntrain_snowpark_df.write.mode(\"overwrite\").save_as_table(\"shift_sales_train\")\n# Save test data\ntest_snowpark_df.write.mode(\"overwrite\").save_as_table(\"shift_sales_test\")\n\n#filter out to only historical data \ntrain_snowpark_df = train_snowpark_df.filter(F.col(\"shift_sales\").is_not_null())\n\n# Define sklearn-like Imputers\nnum_cols=[\"AVG_LOCATION_SHIFT_SALES\"]\nnum_cols_out=[\"AVG_LOCATION_SHIFT_SALES\"]\n\nimpute_cols = SimpleImputer(\n    input_cols=num_cols,\n    output_cols=num_cols_out,\n    strategy=\"constant\",\n    drop_input_cols=True\n    # without specifying fill_value, default is 0\n)\n\n# data_features = impute_cols.fit(data_features).transform(data_features)\n\nohe = OneHotEncoder(\n    input_cols=['SHIFT'], \n    output_cols=['SHIFT_OHE']\n)\n\n# ohe.fit(data_features)\n# data_features = ohe.fit(data_features).transform(data_features)\n\n#filter out to only historical data\n#historical_snowpark_df = data_features.filter(F.col(\"shift_sales\").is_not_null()).drop(\"location_id\", \"city\", \"date\",\"shift\")\n\n#Define Linear Regression\nfeature_cols = [\n    'SHIFT_OHE_AM',\n    'SHIFT_OHE_PM',\n    'AVG_LOCATION_SHIFT_SALES',\n    'MONTH',\n    'DAY_OF_WEEK',\n    'CITY_POPULATION',\n    'LATITUDE',\n    'LONGITUDE'\n]\nlabel_cols = ['SHIFT_SALES']\noutput_cols = ['PREDICTION']\npassthrough_cols = [\n    'LOCATION_ID',\n    'CITY',\n    'DATE',\n    'SHIFT'\n]\n\nlinregression = LinearRegression(\n    input_cols = feature_cols,\n    label_cols = label_cols,\n    output_cols = output_cols,\n    passthrough_cols = passthrough_cols\n)\n\n# Build the pipeline\nmodel_pipeline = Pipeline(\n    steps=[\n        (\"IMPUTE_COLS\",impute_cols),\n        (\"OHE\",ohe),\n        (\"linregression\",linregression)\n    ]\n)\n# Fit the pipeline to the training data\nfitted_pipeline = model_pipeline.fit(train_snowpark_df)\n"
  },
  {
   "cell_type": "code",
   "id": "9741b8b1-fbc1-424d-b7cb-f70953ba1442",
   "metadata": {
    "language": "python",
    "name": "Model_testing",
    "resultHeight": 83,
    "collapsed": false
   },
   "outputs": [],
   "source": "# Test the model\ndf_test_pred = fitted_pipeline.predict(test_snowpark_df)\nactuals = 'SHIFT_SALES'\nprediction = 'PREDICTION'\n\nMSE = mean_squared_error(df=df_test_pred, y_true_col_names=actuals, y_pred_col_names=prediction)\nMAB = mean_absolute_error(df=df_test_pred, y_true_col_names=actuals, y_pred_col_names=prediction)\nR2 = r2_score(df=df_test_pred, y_true_col_name=actuals, y_pred_col_name=prediction)\n\nprint(f'MSE: {MSE}')\nprint(f'MAE: {MAB}')\nprint(f'R2: {R2}')\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b2d8fea7-a12b-478a-b883-7cc3a086ace2",
   "metadata": {
    "collapsed": false,
    "name": "Model_Registry",
    "resultHeight": 504
   },
   "source": [
    "## Model Registry\n",
    "\n",
    "Now, with Snowpark ML's model registry, we have a Snowflake native model versioning and deployment framework. This allows us to log models, tag parameters and metrics, track metadata, create versions, and ultimately deploy models into a Snowflake warehouse or Snowpark Container Service for batch scoring tasks. \n",
    "\n",
    "Snowflake's Model Registry supports SciKitLearn, XGBoost, Pytorch, Tensorflow and MLFlow (via the pyfunc interface) models. Model Registry allows easy deployment of pre-trained open-source models from providers such as HuggingFace.\n",
    "\n",
    "***\n",
    "When creating a model registry, the database name is optional. If you do not specify it, MODEL_REGISTRY is the default. By using different database names, you can create multiple registries in your account for access control, lifecycle management, or other purposes.\n",
    "\n",
    "Add a model by to the registry calling the registry’s `log_model` method. This method:\n",
    "\n",
    "- Serializes the model and uploads it to a Snowflake stage. The model, a Python object, must be serializable (“pickleable”).\n",
    "- Creates an entry in the model registry for the model, referencing the staged location.\n",
    "- Adds metadata such as description and tags to the model as specified in the `log_model` call.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88486e68-9978-44d0-a242-e7d3beca3e75",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "Model_Registry_code",
    "resultHeight": 0
   },
   "outputs": [],
   "source": "from snowflake.ml.registry import Registry\n#suppressed warning\n\nnative_registry = Registry(session, database_name=session.get_current_database(), schema_name=session.get_current_schema())\n\n# Define model name\nmodel_name = \"linear_regression\"\n\n# Let's first log the very first model we trained\nmodel_v1 = native_registry.log_model(\n    model_name=model_name,\n    version_name='V1',\n    model=fitted_pipeline\n)\n\n# Add a description\nmodel_v1.comment = \"This is the model with preprocessing in the pipeline.\""
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b78964-ea03-410d-8d13-911d32aba5ca",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "Add_Metrics",
    "resultHeight": 0
   },
   "outputs": [],
   "source": "df_test_pred_pd = df_test_pred.to_pandas()\nactuals = df_test_pred_pd[[\"SHIFT_SALES\"]]\nprediction = df_test_pred_pd[['PREDICTION']]\n\nmodel_v1.set_metric(metric_name=\"mean_squared_error\", value=MSE)\nmodel_v1.set_metric(metric_name=\"mean_absolute_error\", value=MAB)\nmodel_v1.set_metric(metric_name=\"r2_score\", value=R2)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bcfb7cb-8e41-4905-a7cb-7f92a4a52c03",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "Show_Models",
    "resultHeight": 112
   },
   "outputs": [],
   "source": "model_df = native_registry.show_models()\nmodel_df[['created_on','database_name','schema_name','owner','name','versions']]"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341b7f08-02e0-4e54-89f2-79170fc12696",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "Show_Versions",
    "resultHeight": 112
   },
   "outputs": [],
   "source": [
    "# Let's confirm model(s) that were added\n",
    "native_registry.get_model(model_name).show_versions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e4a90d-f47a-4edc-92ea-e69a4d101b0c",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "Delete_Models",
    "resultHeight": 0
   },
   "outputs": [],
   "source": [
    "# to delete models\n",
    "#m = native_registry.get_model(\"LINEAR_REGRESSION\")\n",
    "#default_version = m.default\n",
    "#m.default = \"V2\"\n",
    "#m.delete_version(\"V1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a584a962-6045-4052-92ac-31106953afcf",
   "metadata": {
    "collapsed": false,
    "name": "Best_Model",
    "resultHeight": 41
   },
   "source": [
    "If you have multiple versions of the model, we want the UDF to be deployed as the version with the highest R2 value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9455cf7e-5ecb-405f-8427-a4ac308d818d",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "Best_Model_code",
    "resultHeight": 54
   },
   "outputs": [],
   "source": "import json \nreg_df = native_registry.get_model(model_name).show_versions()\n\nreg_df[\"r2_score\"] = reg_df[\"metadata\"].apply(\n    lambda x: json.loads(x)[\"metrics\"][\"r2_score\"]\n)\nbest_model = reg_df.sort_values(by=\"r2_score\", ascending=False)\n\ndeployed_version = best_model[\"name\"].iloc[0]\ndeployed_version"
  },
  {
   "cell_type": "markdown",
   "id": "d5846772-46a2-423c-aabb-55741ab6c96b",
   "metadata": {
    "collapsed": false,
    "name": "Registry_Model_Inference",
    "resultHeight": 159
   },
   "source": "**Model Inference**\n\nFinally we are almost ready for prediction! For this, we can look up the latest feature values from Feature Store for the specific data records that we are running prediction on. One of the key benefits of using the Feature Store is that it provides a way to automatically serve up the right feature values during prediction with point-in-time correct feature values. load_feature_views_from_dataset() gets the same feature views used in training, then retrieve_feature_values() lookups the latest feature values.\n\n"
  },
  {
   "cell_type": "code",
   "id": "b36bf84b-376e-4fd6-b25a-99227566e4e4",
   "metadata": {
    "language": "python",
    "name": "Get_Signature_of_model",
    "resultHeight": 44,
    "collapsed": false
   },
   "outputs": [],
   "source": "input_signature = model_v1.show_functions()[0].get(\"signature\").inputs\nprint(input_signature)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6308d5a1-922c-4eb2-acb3-69f2e5074ce6",
   "metadata": {
    "language": "python",
    "name": "get_input_columns",
    "collapsed": false,
    "resultHeight": 0
   },
   "outputs": [],
   "source": "# optional if you only want to use the features required for the model\nreg = Registry(session, database_name=session.get_current_database(), schema_name=session.get_current_schema())\nreg.show_models()\nmv = reg.get_model(model_name).default\ninput_signature = mv.show_functions()[0].get(\"signature\").inputs\ninput_cols = [c.name for c in input_signature]",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "76cabe0a-4704-449e-a30d-288ad32f9872",
   "metadata": {
    "language": "python",
    "name": "Feature_view_slices",
    "collapsed": false,
    "resultHeight": 0
   },
   "outputs": [],
   "source": "# optional if you only want to use the features required for the model\n# Create feature view slices\nds_cols = []\nslice_list = []\nfor fv in [AGGREGATE_WINDOW_FV]:\n    fv_cols = list(fv._feature_desc)\n    slice_cols = [col for col in fv_cols if col not in ds_cols and col in input_cols]\n    if len(slice_cols) > 0:\n        slice_list.append(fv.slice(slice_cols))\n        ds_cols += fv_cols",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a9376e10-1e58-41c1-8a34-ed7071265bb6",
   "metadata": {
    "language": "python",
    "name": "generate_inference_dataset",
    "collapsed": false,
    "resultHeight": 351
   },
   "outputs": [],
   "source": "inference_ds = fs.generate_dataset(\n    name='inference_dataset',\n    spine_df=snowpark_df,\n    features = [AGGREGATE_WINDOW_FV],\n    spine_label_cols=[\"CITY\",\"CITY_POPULATION\",\"LATITUDE\",\"LONGITUDE\",\"SHIFT_SALES\"],\n    desc=\"Inference dataset for linear regression\"\n)\n\ninference_df = inference_ds.read.to_snowpark_dataframe()\ninference_df.show()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b26809e8-20da-4646-b150-ef86f9f15521",
   "metadata": {
    "name": "Model_Inference",
    "collapsed": false,
    "resultHeight": 128
   },
   "source": "## Model Inference\nNow we can use the best model to perform inference using the `run` method specifying the name of the function to be called and passing a Snowpark or pandas DataFrame containing the inference data. "
  },
  {
   "cell_type": "code",
   "id": "4092be22-a310-4fe6-81bf-2b4a8a3d59cd",
   "metadata": {
    "language": "python",
    "name": "Model_Inference_code",
    "collapsed": false,
    "resultHeight": 351
   },
   "outputs": [],
   "source": "# Set the default version to the deployed version (best model)\n# Batch Inference without needing to create a UDF\n\nm = native_registry.get_model(model_name)\nm.default = deployed_version\nmodel_version = m.default\n\nremote_prediction = model_version.run(inference_df, function_name=\"predict\")\nremote_prediction.show()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c731e75-16a5-4d96-a0d3-0bc50d1ff351",
   "metadata": {
    "codeCollapsed": true,
    "collapsed": false,
    "language": "python",
    "name": "cell7",
    "resultHeight": 441
   },
   "outputs": [],
   "source": [
    "st.image('https://raw.githubusercontent.com/Snowflake-Labs/sfguide-tasty-bytes-snowpark-101-for-data-science/211360e75995b670b2b25d3204b59b42e48456cd//assets/model_utilization.png');\n",
    "st.markdown(f\"Now that our model is built and deployed, let's see it in action! We will find the best place to park in Vancouver for tomorrow morning's shift.\");\n",
    "st.image('https://raw.githubusercontent.com/Snowflake-Labs/sfguide-tasty-bytes-snowpark-101-for-data-science/211360e75995b670b2b25d3204b59b42e48456cd//assets/problem_overview.png');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8762a10-fe23-458b-ba86-dd9e54ee90d9",
   "metadata": {
    "collapsed": false,
    "name": "Predict_Next_Shift",
    "resultHeight": 127
   },
   "source": [
    "## Predict Location Sales for the Next Shift\n",
    "\n",
    "We will filter to the morning shift of the first future date in our data and Vancouver. We then call our scoring user-defined function to get predicted shift sales at each location.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29cef355-fcb3-41f1-85bd-68c7deb8585e",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "Predict_Next_Shift_code",
    "resultHeight": 575
   },
   "outputs": [],
   "source": "# Check model predictions for holdout data SHIFT_SALES predictions for Location_IDs in Vancouver\ndate_tomorrow_df = inference_df.filter(\n    (F.col(\"SHIFT_SALES\").isNull())\n    & (F.col(\"SHIFT\") == 'AM')\n    & (F.col(\"CITY\") == 'Vancouver')\n)\n\nresults_pred = model_version.run(date_tomorrow_df, function_name=\"predict\")\nresults_pred.show(20)"
  },
  {
   "cell_type": "markdown",
   "id": "6d51d95d-ddd5-4271-8927-10dab047e9be",
   "metadata": {
    "collapsed": false,
    "name": "Visualize_Maps",
    "resultHeight": 169
   },
   "source": [
    "## Democratize Data Access and Visualize Results on a Map\n",
    "\n",
    "The yellow dots indicate higher predicted sales locations and the purple dots indicate lower predicted sales. We will use this insight to ensure that our drivers are parking at the high-value locations.\n",
    "\n",
    "**Value:** Updated predictions readily available to drive towards our corporate goals.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc574702-6b3c-4eb5-a3cc-57cfdee7d056",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "Visualize_Maps_App",
    "resultHeight": 682
   },
   "outputs": [],
   "source": "from snowflake.snowpark import Window\n\nst.title(\"Select a city to visualize top locations on the map\")\n\nselected_city_map = st.text_input(\"Enter the city 👇\")\n\nsnowpark_df = session.table(\"shift_sales\")\n\n## Create reference to Feature Store\nfs = FeatureStore(\n    session=session,\n    database=session.get_current_database(),\n    name=session.get_current_schema(),\n    default_warehouse=session.get_current_warehouse(),\n    creation_mode=CreationMode.CREATE_IF_NOT_EXIST,\n)\n\ninference_data = fs.get_feature_view(\n    name=\"AGGREGATE_WINDOW_FV\", \n    version='1')\n\ninference_ds = fs.generate_dataset(\n    name='inference_dataset',\n    spine_df=snowpark_df,\n    features = [AGGREGATE_WINDOW_FV],\n    spine_label_cols=[\"CITY\",\"CITY_POPULATION\",\"LATITUDE\",\"LONGITUDE\",\"SHIFT_SALES\"],\n    desc=\"Inference dataset for linear regression\"\n)\n\ninference_df = inference_ds.read.to_snowpark_dataframe()\n\nlocation_predictions_df = inference_df.filter(\n    (F.col(\"SHIFT_SALES\").isNull())\n    & (F.col(\"SHIFT\") == 'AM')\n    & (F.col(\"city\")==selected_city_map)\n)\n\n# Get predictions\nnative_registry = Registry(session, database_name=\"FROSTBYTE_TASTY_BYTES\", schema_name=\"SCHEMA_MICHELLE\")\nmv = native_registry.get_model(\"linear_regression\").default\n\nlocation_predictions_df = mv.run(function_name=\"predict\", X=location_predictions_df).select(\n    \"city\",\n    \"location_id\", \n    \"latitude\", \n    \"longitude\",\n    \"prediction\"\n)\n\nwindow = Window.partitionBy(location_predictions_df['city']).orderBy(location_predictions_df['prediction'].desc())\nfiltered_df = location_predictions_df.select(\n    \"city\",\n    \"location_id\", \n    \"latitude\", \n    \"longitude\",\n    \"prediction\",\n    F.rank().over(window).alias('rank')).filter(F.col('rank') <= 20)\n                                               \n# Pull location predictions into a pandas DataFrame\npredictions_df = filtered_df.to_pandas()\n\nst.map(predictions_df,\n    latitude='latitude',\n    longitude='longitude',\n    size='prediction',\n)\n\n\n"
  },
  {
   "cell_type": "markdown",
   "id": "cbfdc6d0-1237-4500-98db-12a222895d42",
   "metadata": {
    "collapsed": false,
    "name": "Next_Steps",
    "resultHeight": 647
   },
   "source": [
    "# Next Steps\n",
    "\n",
    "\n",
    "\n",
    "We've created features, built a base model, and deployed it - all with Snowflake. What else can we do from here?\n",
    "\n",
    "\n",
    "### 1. Add new features from __Snowflake Data Marketplace:\n",
    "\n",
    "- [Snowflake Marketplace](https://www.snowflake.com/snowflake-marketplace/),\n",
    "- [Free demo listing: Weather Source, LLC: Frostbyte](https://www.snowflake.com/datasets/weather-source-llc-frostbyte/),\n",
    "- [Free demo Listing: Safegraph](https://app.snowflake.com/marketplace/listing/GZSNZL1CN82/safegraph-safegraph-frostbyte?search=frostbyte),\n",
    "\n",
    "### 2. Improve model performance using a User-Defined Table Function (UDTF) for parallel hyperparameter tuning to identify the optimal combination of hyperparameters to use in training:[¶](http://localhost:8889/notebooks/tasty_bytes_snowpark_101-release1.ipynb#2.-Improve-model-performance-using-a-User-Defined-Table-Function-(UDTF)-for-parallel-hyperparameter-tuning-to-identify-the-optimal-comibination-of-hyperparameters-to-use-in-training:)\n",
    "\n",
    "- [Blog: Parallel Hyperparameter Tuning Using Snowpark](https://medium.com/snowflake/parallel-hyperparameter-tuning-using-snowpark-53cdec2faf77),\n",
    "- [Documentation: Implementing User-Defined Table Functions (UDTFs) in Python](https://docs.snowflake.com/en/developer-guide/udf/python/udf-python-tabular-functions.html),\n",
    "\n",
    "### 3. Automate predictions with __Streams & Tasks__:\n",
    "\n",
    "- [Quickstart: Getting Started with Streams & Tasks](https://quickstarts.snowflake.com/guide/getting_started_with_streams_and_tasks/index.html?index=..%2F..index#0),\n",
    "- [Documentation: Introduction to Tasks](https://docs.snowflake.com/en/user-guide/tasks-intro.html),\n",
    "- [Documentation: Introduction to Streams](https://docs.snowflake.com/en/user-guide/streams-intro.html),\n",
    "\n",
    "### 4. Create an app interface with __Streamlit__ for truck drivers to get location predictions:\n",
    "\n",
    "- [Quickstart: Getting Started with Snowpark for Python and Streamlit](https://quickstarts.snowflake.com/guide/getting_started_with_snowpark_for_python_streamlit/index.html?index=..%2F..index#0),\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf04f22e-34ce-41f1-bdf2-ae1410b39256",
   "metadata": {
    "collapsed": false,
    "name": "Reset",
    "resultHeight": 141
   },
   "source": [
    "# Reset\n",
    "\n",
    "We will run the following cell to remove the objects from Snowflake that we created in this notebook. Dropping our stage will remove the model file. We will also close our Snowflake session.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e15ab24-11a8-4ba4-b71c-ab67e3d5049b",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "Reset_code"
   },
   "outputs": [],
   "source": [
    "# Drop training table\n",
    "#session.sql(\"DROP TABLE IF EXISTS shift_sales_train\").collect()\n",
    "\n",
    "# Drop testing table\n",
    "#session.sql(\"DROP TABLE IF EXISTS shift_sales_test\").collect()\n",
    "\n",
    "#session.sql(\"DROP SCHEMA IF EXISTS \"+ MY_SCHEMA).collect()\n",
    "\n",
    "\n",
    "# Drop training stored procedure\n",
    "#session.sql(\"DROP PROCEDURE IF EXISTS sproc_train_linreg(varchar, array, varchar, varchar)\").collect()\n",
    "\n",
    "# Drop inference user-defined function\n",
    "#session.sql(\n",
    "#    \"DROP FUNCTION IF EXISTS udf_linreg_predict_location_sales(float, float, float, float, float, float, float, float)\"\n",
    "#).collect()\n",
    "\n",
    "# Drop stage\n",
    "#session.sql(\"DROP STAGE IF EXISTS model_stage\").collect()\n",
    "\n",
    "# Scale down compute\n",
    "#session.sql(\"DROP WAREHOUSE \" + MY_WAREHOUSE).collect()\n",
    "\n",
    "# Close the session\n",
    "#session.close()"
   ]
  }
 ]
}